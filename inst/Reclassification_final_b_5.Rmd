---
title: "Reclassification"
author: "Rainer M Krug"
date: "06/05/2022"
output:
  html_document:
    dev: png
    fig_width: 10
    fig_height: 12
    toc: true
    toc_float: true
    toc_collapsed: true
    code_folding: hide
params:
  output_dir: ~/reclassification/
---

```{r setup_reclass, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  message = TRUE,
  warning = TRUE
)
# library("LEEF.analysis")
```

# Preparation
## Set Base to use
This reclassification is based on the classifiers as in the repo at the base URL which speicifies the repo and the reference / tag which should be used for this reclassification
```{r}
base <- paste0("https://github.com/LEEF-UZH/LEEF.parameter/raw/", "v1.6.1", "/")
```

## Set output path
```{r output_path}
output <-  params$output
dir.create(output, recursive = TRUE, showWarnings = FALSE)
```


## Set Archive Directory
Make sure, that the S3 LEEF archive is mounted and you know the path to the archive as it is needed.

```{r setArchive}
archive_dir <- "~/Duck/LEEFSwift3/"
if (!file.exists(archive_dir)){
  stop(
    "Archive directory '", archive_dir, "' does not exist.\n",
    "Probably not mounted?" 
  )
}
```

## Set number of cores to be used
```{r setCores}
cores <- 7
```

## Set timestamps
All available after `20220504` data will be reclassified and are determined below based on the archived data.

## Set Bottle and range of timestamps
```{r bot_after_until}
bott <- "b_02"
after <- 20200000
until <- 30000000
```

# Reclassification 

## Flowcam

### Prepare Flowcam
```{r prepare_flowcam}
pdir <- tempfile()
dir.create(pdir, showWarnings = FALSE, recursive = TRUE)

class_constant <- file.path(pdir, "class_constant.rds")
download.file(
  paste0(base, "parameter/flowcam/svm_flowcam_classifiers_B02_contaminated_after_20220504_20220710_MergedData.rds"), 
  destfile = class_constant,
  mode = "wb"
)

class_increasing <- file.path(pdir, "class_increasing.rds")
download.file(
  paste0(base, "parameter/flowcam/svm_flowcam_classifiers_B02_contaminated_after_20220504_20220710_MergedData.rds"), 
  destfile = class_increasing,
  mode = "wb"
)

timestamps <- list.files(
  path = file.path(archive_dir, "LEEF.archived.data", "LEEF", "3.archived.data", "extracted"), 
  pattern = "^LEEF\\.fast\\.flowcam\\.",
  recursive = FALSE, 
  full.names = FALSE
)
timestamps <- gsub("^LEEF\\.fast\\.flowcam\\.", "", timestamps)
timestamps <- as.integer(timestamps)

timestamps <- timestamps[(timestamps >= after) & (timestamps <= until)]
timestamps
```

### Classify Flowcam
```{r classify_flowcam}
out <- file.path(output, "flowcam")
if (!file.exists(out)) {
  message("Classifying flowcam...")
  system.time(
    classify_flowcam_archive(
      archive_dir = archive_dir, 
      timestamps = timestamps, 
      algae_traits_name = "algae_traits_filtered.rds", 
      classifier_constant_name = class_constant, 
      classifier_increasing_name = class_increasing, 
      species_tracked = yaml::read_yaml(paste0(base, "parameter/flowcam/flowcam.yml"))$species_tracked,
      output = out,
      mc.cores = cores,
      bottle = bott
    )
  )
} else {
  message(
    "Directory ", out, "exists\n",
    "skipping Flowcam."
  )
}
```

### Clean Up Flowcam
```{r cleanup_flowcam}
unlink(pdir)
rm(class_constant, class_increasing, out)
```

# Add to database
```{r addRRD}

files <- list.files(file.path(output, "flowcam"), full.names = TRUE)
tables <- paste0(
  basename("flowcam"), "__",
  sapply( files, function(f) {strsplit(basename(f), "\\.")[[1]][[1]]})
)
names(tables) <- NULL

delete_density_sql <- paste0(
  "DELETE ",
  "FROM flowcam__algae_density ", 
  "WHERE bottle = '", bott, "' AND timestamp > ", format(after, scientific = FALSE), " AND timestamp < ", format(until, scientific = FALSE)
)
delete_traits_sql <- paste0(
  "DELETE ",
  "FROM flowcam__algae_traits ", 
  "WHERE bottle = '", bott, "' AND timestamp > ", format(after, scientific = FALSE), " AND timestamp < ", format(until, scientific = FALSE)
)

db <- paste0(output, "LEEF.RRD.reclassified.sqlite")

conn <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  db = db
)

DBI::dbExecute(conn, delete_density_sql)
DBI::dbExecute(conn, delete_traits_sql)
DBI::dbDisconnect(conn)

message("Adding directory 'flowcam' to the database")

system.time(
  add_to_db(
    fns = files,
    tables = tables,
    db = db,
    remove_timestamps = NULL,
    check_timestamps = FALSE
  )
)



```

# Add experiment specific tables
```{r addExptables}
pdir <- tempfile()
dir.create(pdir, showWarnings = FALSE, recursive = TRUE)

experimental_design <- file.path(pdir, "experimental_design.csv")
download.file(
  paste0(base, "parameter/00.general.parameter/experimental_design.csv"), 
  destfile = experimental_design,
  mode = "wb"
)

compositions <- file.path(pdir, "compositions.csv")
download.file(
  paste0(base, "parameter/00.general.parameter/compositions.csv"), 
  destfile = compositions,
  mode = "wb"
)

light_decline_schedule <- file.path(pdir, "light_decline_schedule.csv")
download.file(
  paste0(base, "treatment/light_decline_schedule.csv"), 
  destfile = light_decline_schedule,
  mode = "wb"
)

immigration_schedule <- file.path(pdir, "immigration_schedule.csv")
download.file(
  paste0(base, "treatment/immigration_schedule.csv"), 
  destfile = immigration_schedule,
  mode = "wb"
)


add_experiment_tables(
  db = file.path(output, "LEEF.RRD.reclassified.sqlite"), 
  composition = compositions, 
  experimetal_design = experimental_design,
  light_decline_schedule = light_decline_schedule,
  immigration_schedule = immigration_schedule,
  overwrite = TRUE
)

unlink(pdir)
rm(pdir, compositions, experimental_design)
```

# Create Diagnostic Report
```{r createDiagReport}
report_diagnostic(
  db = file.path(output, "LEEF.RRD.reclassified.sqlite"), 
  suffix = "reclassified", 
  format = "html"
)
```

